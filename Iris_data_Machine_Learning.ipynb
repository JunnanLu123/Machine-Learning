{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Iris Classification with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we build and train a model to categorize the Iris flower base the sample data. The algorithm we chose to implement for this project is multilayer perceptron. The Iris data set is available from the University of California at Irvine (UCI) Machine Learning Repository. The dataset consists of information on 150 Iris flowers. We focus on learning three Iris species which are Setosa, Versicolour, and Virginica. The dataset is characterized by five attributes:\n",
    "    1. Sepal Length in centimeters\n",
    "    2. Sepal Width in centimeters\n",
    "    3. Petal Length in centimeters\n",
    "    4. Petal Width in centimeters\n",
    "    5. Targeting Class (Setosa, Vericolour, Virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![setosa](/images/iris-setosa-1.jpeg)|![versicolor](/images/Iris_versicolor_3.jpg)|![virginica](/images/iris_virginica.jpeg)|\n",
    "|:---|:---:|---:|\n",
    "|Iris Setosa|Iris Versicolor|Iris Virginica|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We designed a Neural Network with very simple structure. The network contain 4 layers which are input, hidden, and output layers. The input layers consists of four nodes. The input for the network is a vector of Iris features which are Sepal Length, Sepal Width, Petal Length, and Petal Width. There are two hidden layers with 10 nodes on each of the layers. We chose to use linear function, Relu, as the activation functions. The predicted probability of the Iris class will send to the output layer which represents Iris flower classes. The figure below shows the model Neuro Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeuralNet.png](/images/NeuralNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various types of Neuro Networks. In this project, we chose to build the fully connected Neuro Networks, also known as Dense networks. The neurons from the input layer are connected each neurons in the first hidden layer. The same way applys to second and final output layers. There is a single bias unit that is connected to each neurons in the hidden layers. The neurons in the hidden layers compute the weighted sum from the inputs to form the scalar net activation. We write the equation as in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{y} = \\mathbf{\\sum_{i=1}^n x_i w_i} + \\mathbf{bias}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subscript i indexes neurons in the input layer. The w denotes the input to hidden layer weights at the hidden layer neurons. Such weights are also named synapses and the values of the connections the synaptic weights. The output y can be thought of as a function of input feature vector x. When there are k output neurons, we can think of the network as computing k discriminant functions and classify the input according to which discriminant function is the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we express the Neuro Network to Mathematical functions as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{\\output} = \\mathbf{\\left(\\sum_{j=1}^n w_j_m\\left(\\sum_{i=1}^n w_i_n\\left(\\sum_{q=1}^n w_q_o x + bias_q\\right) + bias_n\\right)+bias_m\\right)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wjm here denotes the weights in layer m and Win denotes the weights in layer n. The bias_q, bias_n, and bias_m denote the bias vector in layer q, n, and m accordingly. The output is a vector of dimension of three which represents the probability of the three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error for the model that we build and train for this project to be the least mean square error which is sum over the output neurons of the squared difference between the actual desired value z and the actual ouput t. The equation is denoted below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{E\\left(w\\right)} = \\mathbf{\\frac{1}{2}} \\mathbf{\\sum_{k=1}^n \\left(z_k-t_k\\right)^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where z and t are the desired value and actual network output. The purpose for applying least mean square error and gradient descent is to train the model learning weights. The weights are initialized with value zeros, and then changed in a direction that will reduce error:\n",
    "\\begin{align}\n",
    "    \\mathbf{\\nabla w} = mathbf{-n} mathbf{\\frac{\\partial\\mathbf{E}}{\\partial\\w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the learning rate which indicates the relative size of the change in weights. The weight values is updated as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "        \\mathbf{W(k+1)}=\\mathbf{W(k)} \\mathbf{-n} \\frac{\\partial\\mathbf{E}}{\\mathbf{\\partial w}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the error function is given analytically, it is differentiable. The partial derivative with respect to w is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial{\\mathbf{E}}}{\\partial\\mathbf{w}}=-\\mathbf{(z-w^t x)x}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting the result into the weight update equation, the resulting function is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{W(k+1)}=\\mathbf{W(k)} - \\mathbf{n(z-w^tx)x}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the learning rate, range 0<n<2 for the learning process to converge. In practice, we iterate the learning process to a specified threshold. The final computation in the hidden layers is the activation function. We chose the rectifier linear unit (ReLU) as the activation function for our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we download the training dataset from \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\" and store it onto the local hard drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the tf.data.experimental.make_csv_dataset to parse the training dataset. The output is the file consisting a tuple of Iris features and corresponding labels. The size of the file is the number of rows in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total 120 rows of Iris features.\n",
    "# Chosing first 32 rows of data for training.\n",
    "\n",
    "train_data_location = \"C:/Users/JunnanLu123/iris_training.csv\"\n",
    "train_data_filePt = (train_file_fp, 32, column_names = column_names, label_name = label_name, num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns of the data set.\n",
    "\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "# The first four fields are flower features \n",
    "# representing flower measurements.\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "\n",
    "# The last column is the flower label which we\n",
    "# hope the model could predict in high accuracy.\n",
    "\n",
    "label_name = column_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each class is associated with the string name such as setosa.\n",
    "# The class name are mapped to then list representation such as\n",
    "# 0, Iris setosa\n",
    "# 1, Iris versicolor\n",
    "# 2, virginica\n",
    "\n",
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data is a file of CSV format. Therefore, we call\n",
    "# tf.data.experimental.make_csv_dataset to make the training\n",
    "# dataset. The make_csv_dataset returns a tf.data.Dataset which\n",
    "# consists of feature and label pairs.\n",
    "\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(train_file_fp, 32, column_names = column_names, label_name = label_name, num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train_dataset object is iterable. The return value is a tuple \n",
    "# of features and label pair. The label here denotes the flower\n",
    "# classes.\n",
    "\n",
    "self.features, self.label = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the sample data is corresponding to the feature array which is grouped together in a batch. We set the default batch size to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call tf.stack to create a combined tensor at \n",
    "# the specified dimension. And we pack the features\n",
    "# into a single array.\n",
    "\n",
    "def pack(self, features, label):\n",
    "    self.features = tf.stack(list(features.values()), axis=1)\n",
    "return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each features such as (feature, label) pair mapped into training\n",
    "# dataset by using tf.data.Dataset.map.\n",
    "\n",
    "dataset = dataset.map(pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the figure to visualize the flower features. The batch size is 32 rows of feature and label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the flower data from the dataset batch.\n",
    "### The batch size is 32 rows of feature and label pairs.\n",
    "\n",
    "plt.scatter(self.features['petal_length'], self.features['petal_width'], c=self.label, cmap='viridis')\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure on the left displays the distribution of features for sepal length and sepal width. The figure on the right displays the features for petal length and petal width. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![sepal][/images/Feature1.png]|![petal][/images/Feature2.png]|\n",
    "|:---|---:|\n",
    "|Sepal Length and Width|Petal Length and Width|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
