{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Iris Classification with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we build and train a model to categorize the Iris flower base the sample data. The algorithm we chose to implement for this project is multilayer perceptron. The Iris data set is available from the University of California at Irvine (UCI) Machine Learning Repository. The dataset consists of information on 150 Iris flowers. We focus on learning three Iris species which are Setosa, Versicolour, and Virginica. The dataset is characterized by five attributes:\n",
    "    1. Sepal Length in centimeters\n",
    "    2. Sepal Width in centimeters\n",
    "    3. Petal Length in centimeters\n",
    "    4. Petal Width in centimeters\n",
    "    5. Targeting Class (Setosa, Vericolour, Virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![setosa](/images/iris-setosa-1.jpeg)|![versicolor](/images/Iris_versicolor_3.jpg)|![virginica](/images/iris_virginica.jpeg)|\n",
    "|:---|:---:|---:|\n",
    "|Iris Setosa|Iris Versicolor|Iris Virginica|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We designed a Neural Network with very simple structure. The network contain 4 layers which are input, hidden, and output layers. The input layers consists of four nodes. The input for the network is a vector of Iris features which are Sepal Length, Sepal Width, Petal Length, and Petal Width. There are two hidden layers with 10 nodes on each of the layers. We chose to use linear function, Relu, as the activation functions. The predicted probability of the Iris class will send to the output layer which represents Iris flower classes. The figure below shows the model Neuro Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeuralNet.png](/images/NeuralNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various types of Neuro Networks. In this project, we chose to build the fully connected Neuro Networks, also known as Dense networks. The neurons from the input layer are connected each neurons in the first hidden layer. The same way applys to second and final output layers. There is a single bias unit that is connected to each neurons in the hidden layers. The neurons in the hidden layers compute the weighted sum from the inputs to form the scalar net activation. We write the equation as in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\y = \\sum_{i=1}^n x_i w_i + \\bias\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subscript i indexes neurons in the input layer. The w denotes the input to hidden layer weights at the hidden layer neurons. Such weights are also named synapses and the values of the connections the synaptic weights. The output y can be thought of as a function of input feature vector x. When there are k output neurons, we can think of the network as computing k discriminant functions and classify the input according to which discriminant function is the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we express the Neuro Network to Mathematical functions as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\output = \\left(\\sum_{j=1}^n w_j_m\\left(\\sum_{i=1}^n w_i_n\\left(\\sum_{q=1}^n w_q_o x + bias_q\\right) + bias_n\\right)+bias_m\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wjm here denotes the weights in layer m and Win denotes the weights in layer n. The bias_q, bias_n, and bias_m denote the bias vector in layer q, n, and m accordingly. The output is a vector of dimension of three which represents the probability of the three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error for the model that we build and train for this project to be the least mean square error which is sum over the output neurons of the squared difference between the actual desired value z and the actual ouput t. The equation is denoted below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\E\\left(w\\right) = \\frac{1}{2} \\sum_{k=1}^n \\left(z_k-t_k\\right)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where z and t are the desired value and actual network output. The purpose for applying least mean square error and gradient descent is to train the model learning weights. The weights are initialized with value zeros, and then changed in a direction that will reduce error:\n",
    "\\begin{align}\n",
    "\\nabla w = \\-n\\,\\frac{\\partial\\mathbf{E}}{\\partialw}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the learning rate which indicates the relative size of the change in weights. The weight values is updated as follow:\n",
    "\\begin{ailgn}\n",
    "\\w\\left(k+1\\right) = \\w\\left(k\\right) - \\a\\,\\frac{\\partial\\mathbf{E}}{\\partialw}\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
