{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Iris Classification with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we build and train a model to categorize the Iris flower base the sample data. The algorithm we chose to implement for this project is multilayer perceptron. The Iris data set is available from the University of California at Irvine (UCI) Machine Learning Repository. The dataset consists of information on 150 Iris flowers. We focus on learning three Iris species which are Setosa, Versicolour, and Virginica. The dataset is characterized by five attributes:\n",
    "    1. Sepal Length in centimeters\n",
    "    2. Sepal Width in centimeters\n",
    "    3. Petal Length in centimeters\n",
    "    4. Petal Width in centimeters\n",
    "    5. Targeting Class (Setosa, Vericolour, Virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![setosa](/images/iris-setosa-1.jpeg)|![versicolor](/images/Iris_versicolor_3.jpg)|![virginica](/images/iris_virginica.jpeg)|\n",
    "|:---|:---:|---:|\n",
    "|Iris Setosa|Iris Versicolor|Iris Virginica|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We designed a Neural Network with very simple structure. The network contain 4 layers which are input, hidden, and output layers. The input layers consists of four nodes. The input for the network is a vector of Iris features which are Sepal Length, Sepal Width, Petal Length, and Petal Width. There are two hidden layers with 10 nodes on each of the layers. We chose to use linear function, Relu, as the activation functions. The predicted probability of the Iris class will send to the output layer which represents Iris flower classes. The figure below shows the model Neuro Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeuralNet.png](/images/NeuralNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various types of Neuro Networks. In this project, we chose to build the fully connected Neuro Networks, also known as Dense networks. The neurons from the input layer are connected each neurons in the first hidden layer. The same way applys to second and final output layers. There is a single bias unit that is connected to each neurons in the hidden layers. The neurons in the hidden layers compute the weighted sum from the inputs to form the scalar net activation. We write the equation as in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{y} = \\mathbf{\\sum_{i=1}^n x_i w_i} + \\mathbf{bias}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subscript i indexes neurons in the input layer. The w denotes the input to hidden layer weights at the hidden layer neurons. Such weights are also named synapses and the values of the connections the synaptic weights. The output y can be thought of as a function of input feature vector x. When there are k output neurons, we can think of the network as computing k discriminant functions and classify the input according to which discriminant function is the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we express the Neuro Network to Mathematical functions as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{\\output} = \\mathbf{\\left(\\sum_{j=1}^n w_j_m\\left(\\sum_{i=1}^n w_i_n\\left(\\sum_{q=1}^n w_q_o x + bias_q\\right) + bias_n\\right)+bias_m\\right)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wjm here denotes the weights in layer m and Win denotes the weights in layer n. The bias_q, bias_n, and bias_m denote the bias vector in layer q, n, and m accordingly. The output is a vector of dimension of three which represents the probability of the three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error for the model that we build and train for this project to be the least mean square error which is sum over the output neurons of the squared difference between the actual desired value z and the actual ouput t. The equation is denoted below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{E\\left(w\\right)} = \\mathbf{\\frac{1}{2}} \\mathbf{\\sum_{k=1}^n \\left(z_k-t_k\\right)^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where z and t are the desired value and actual network output. The purpose for applying least mean square error and gradient descent is to train the model learning weights. The weights are initialized with value zeros, and then changed in a direction that will reduce error:\n",
    "\\begin{align}\n",
    "    \\mathbf{\\nabla w} = mathbf{-n} mathbf{\\frac{\\partial\\mathbf{E}}{\\partial\\w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the learning rate which indicates the relative size of the change in weights. The weight values is updated as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "        \\mathbf{W(k+1)}=\\mathbf{W(k)} \\mathbf{-n} \\frac{\\partial\\mathbf{E}}{\\mathbf{\\partial w}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the error function is given analytically, it is differentiable. The partial derivative with respect to w is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial{\\mathbf{E}}}{\\partial\\mathbf{w}}=-\\mathbf{(z-w^t x)x}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting the result into the weight update equation, the resulting function is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{W(k+1)}=\\mathbf{W(k)} - \\mathbf{n(z-w^tx)x}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the learning rate, range 0<n<2 for the learning process to converge. In practice, we iterate the learning process to a specified threshold. The final computation in the hidden layers is the activation function. We chose the rectifier linear unit (ReLU) as the activation function for our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we download the training dataset from \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\" and store it onto the local hard drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the tf.data.experimental.make_csv_dataset to parse the training dataset. The output is the file consisting a tuple of Iris features and corresponding labels. The size of the file is the number of rows in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total 120 rows of Iris features.\n",
    "# Chosing first 32 rows of data for training.\n",
    "\n",
    "train_data_location = \"C:/Users/JunnanLu123/iris_training.csv\"\n",
    "train_data_filePt = (train_file_fp, 32, column_names = column_names, label_name = label_name, num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns of the data set.\n",
    "\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "# The first four fields are flower features \n",
    "# representing flower measurements.\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "\n",
    "# The last column is the flower label which we\n",
    "# hope the model could predict in high accuracy.\n",
    "\n",
    "label_name = column_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each class is associated with the string name such as setosa.\n",
    "# The class name are mapped to then list representation such as\n",
    "# 0, Iris setosa\n",
    "# 1, Iris versicolor\n",
    "# 2, virginica\n",
    "\n",
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data is a file of CSV format. Therefore, we call\n",
    "# tf.data.experimental.make_csv_dataset to make the training\n",
    "# dataset. The make_csv_dataset returns a tf.data.Dataset which\n",
    "# consists of feature and label pairs.\n",
    "\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(train_file_fp, 32, column_names = column_names, label_name = label_name, num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train_dataset object is iterable. The return value is a tuple \n",
    "# of features and label pair. The label here denotes the flower\n",
    "# classes.\n",
    "\n",
    "self.features, self.label = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the sample data is corresponding to the feature array which is grouped together in a batch. We set the default batch size to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call tf.stack to create a combined tensor at \n",
    "# the specified dimension. And we pack the features\n",
    "# into a single array.\n",
    "\n",
    "def pack(self, features, label):\n",
    "    self.features = tf.stack(list(features.values()), axis=1)\n",
    "return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each features such as (feature, label) pair mapped into training\n",
    "# dataset by using tf.data.Dataset.map.\n",
    "\n",
    "dataset = dataset.map(pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the figure to visualize the flower features. The batch size is 32 rows of feature and label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the flower data from the dataset batch.\n",
    "# The batch size is 32 rows of feature and label pairs.\n",
    "\n",
    "plt.scatter(self.features['petal_length'], self.features['petal_width'], c=self.label, cmap='viridis')\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure on the left displays the distribution of features for sepal length and sepal width. The figure on the right displays the features for petal length and petal width. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![sepal](/images/Feature1.png)|![petal](/images/Feature2.png)|\n",
    "|:---|:---:|\n",
    "|Sepal Length and Width|Petal Length and Width|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Training With Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the model layers with tf.keras API. The API provides the way to handle the complexity of connections among model layers. The layer is the fundamental data structure for processing the Iris data for our project. In this project, our network topology is that layers are densly connected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorflow and Keras modules\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each layer there is only one input tensor and one output tensor. If the topology is non-linear which there is a residual connection or needing layer sharing, then, our model will not produce the optimal results. Therefore, we apply the tf.keras.Sequential() model which is a linear model of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create layers that only accept input of 2D tensors\n",
    "# where the input dimension is 4xNone. The axis 0 could \n",
    "# be any value since it is not specified. The first \n",
    "# hidden layer requires an input with dimension of\n",
    "# 10. And the same way goes for the second hidden layer\n",
    "# The final output layer will return a tensor with\n",
    "# dimension of 3 denoting Setosa, Versicolor, and\n",
    "# Verginica respectively.\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Here we chose Rectifier linear Unit, ReLu, as the activation\n",
    "# function. \n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss and Gradient Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we use gradient descent algorithm to update weights of the layers to minimize the computing errors. We calculate the error by using tf.keras.losses.SparseCategoricalCrossentropy() function. The loss function returns the average loss across the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the sparse categorical crossentropy loss\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tf.GradientTape to record automatic differetiation. In our model, the trainable variable are the weights and bias which are being watched automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientTape automatically watch trainable variable,\n",
    "# the resource will be released when GradientTape.gradient()\n",
    "# method is called. Using gradient to calculate and optimize \n",
    "# gradients of the model.\n",
    "\n",
    "with tf.GradientTape() as Tape:\n",
    "   prediction = model(features, training=True)\n",
    "   loss = loss_object(label, prediction)\n",
    "grads = Tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create The Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to minimize the loss function defined above. We choose to use the tf.keras.optimizers.SGD() to update the set of weights and bias in our network. The gradient descent method is commonly used to update changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the optimizer to compute loss and set the learning\n",
    "# rate to 0.01\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for our model is to learn from the training data and then is able to predict unseen data with high accuracy. Overfitting problem occurs when model is producing high accurate resutls when feeding with training data but performing poorly when feeding unseen data. The training process consists 100 epochs. We iterate through each epoch. And for each of the epochs, we then iterate through each of the 32 training dataset to let model to make predictions. Then, applying the loss and optimization methods mentioned earlier to update the weights and bias of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Process Using Batch of 32 Samples\n",
    "# Each epoch is one pass through the dataset\n",
    "\n",
    "\n",
    "# Keep tracking the value for plotting.\n",
    "loss_value_list=[]\n",
    "model_accuracy_list=[]\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss_value = tf.keras.metrics.Mean()\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    # Training the model through each of the training sample\n",
    "    # Iterate through each example of the feature and label of\n",
    "    # the dataset. \n",
    "\n",
    "    for features, label in dataset:\n",
    "    # GradientTape automatically watch trainable variable,\n",
    "    # the resource will be released when GradientTape.gradient()\n",
    "    # method is called. Using gradient to calculate and optimize \n",
    "    # gradients of the model.\n",
    "\n",
    "        with tf.GradientTape() as Tape:\n",
    "             prediction = model(features, training=True)\n",
    "             loss = loss_object(label, prediction)\n",
    "        grads = Tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # Optimizer implemented the Stochastic Gradient Decent method.\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # Tracking the stats of the loss and accuracy value for visualization\n",
    "\n",
    "        loss_value.update_state(loss)\n",
    "        accuracy.update_state(label, model(features, training=True))\n",
    "                \n",
    "    # Append the loss value and accuracy result to the lists\n",
    "\n",
    "    loss_value_list.append(loss_value.result())\n",
    "    model_accuracy_list.append(accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the plot by using matplotlib module.\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss in time elapsed fashion.\n",
    "# TensorBoard can also help to visualize the training metrics.\n",
    "# We are using matplotlib to create basic metrics chart.\n",
    "\n",
    "fig1, axes = plt.subplots(2, sharex=True, figsize=(12,8))\n",
    "fig1.suptitle(\"Train Metrics\")\n",
    "axes[0].set_ylabel(\"LOSS\", fontsize=14)\n",
    "axes[0].plot(loss_value_list)\n",
    "\n",
    "axes[1].set_ylabel(\"ACCURACY\", fontsize=14)\n",
    "axes[1].set_xlabel(\"EPOCHS\", fontsize=14)\n",
    "axes[1].plot(model_accuracy_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures below present training loss and accuracy over time. We are happy that figures show no suprising behaviour from our model. The loss and accuracy values are satisifying our expactations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![LOSS](/images/LossFig.png)|![ACCURACY](/images/Accuracy.png)|\n",
    "|:---|---:|\n",
    "|Training loss|Training Accuracy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained our model. Now, we try to get the metrics by testing the model with testing data. Similar to model training, we download the test dataset and process the test data with tf.data.experimental.make_csv_dataset to parse the testing data. However, the difference is that, when in model testing phase, we set the training parameter in model to false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Testing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the test data from https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv. Processing testing data is similar to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation using test data\n",
    "# Using single epoch to test the model.\n",
    "# Dataset is downloaded and store in local machine.\n",
    "# Parsing the test data.\n",
    "\n",
    "dataset = self.makeDataCSV(\"C:/Users/JunnanLu123/iris_test.csv\")\n",
    "dataset = dataset.map(self.pack2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike training the model, we iterate through each row of the testing data in one epoch and compute the predictions. Then, we compare the model output to the actual value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates how often predictions equals labels.\n",
    "\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "test_accuracy_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We iterate every sample from the dataset and compare to the\n",
    "# model's prediction against the actual to measure model's \n",
    "# accuracy across the entire test dataset.\n",
    "\n",
    "for features, label in dataset:\n",
    "    prediction = model(features, training=False)\n",
    "    prediction = tf.nn.softmax(prediction, axis=1,name='softmax')\n",
    "    prediction = tf.argmax(prediction, axis=1, output_type=tf.dtypes.int32)\n",
    "    print (prediction)\n",
    "    accuracy.update_state(label, prediction)\n",
    "    print (accuracy.result())\n",
    "    test_accuracy_list.append(accuracy.result())\n",
    "print (\"TEST Accuracy {:1.2%}\".format(accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model archives test accuracy at 93.33% after 100 epchos of model training loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Trained Model to Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to use our trained model to compute some unlabeled data. The dataset is downloaded from \"https://gist.github.com/curran/a08a1080b88344b0c8a7#file-iris-csv\". We parse and extract the feature data from the dataset similar to methods mentioned earlier. The only difference is that we choose first 10 rows of data to make a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making data from the CSV file with specified batch size\n",
    "# The tf.data.experimental.make_csv_dataset method return\n",
    "# parsed dataset\n",
    "\n",
    "def makeDataFromCSV(self, name, batchSize):\n",
    "    # Specified column names to make a suitable format\n",
    "    column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "    feature_names = column_names[:-1]\n",
    "    label_name = column_names[-1]\n",
    "    dataset = tf.data.experimental.make_csv_dataset(name, batch_size=batchSize, column_names = column_names, label_name = label_name, num_epochs = 1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from the dataset. In here,\n",
    "# we take 10 samples from the features data\n",
    "# letting our model to predict\n",
    "\n",
    "predict_data = makeDataFromCSV(\"C:/Users/Byzantin/source/repos/Data1/Data1/iris1.csv\",10)\n",
    "predict_data = predict_data.map(self.pack2)\n",
    "features, labels = unpackData(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for making the feature and \n",
    "# label batch\n",
    "\n",
    "def unpackData(self, dataset):\n",
    "    features, labels = next(iter(dataset))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of three class names\n",
    "# Labeled data mapped with list index.\n",
    "\n",
    "class_name = ['Iris Setosa', 'Iris Versicolor', 'Iris Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the trained model to predict unlabeled data.\n",
    "# prediction stores model result.\n",
    "prediction = model(features, training=False)\n",
    "\n",
    "# Iterating through the model result.\n",
    "for index, logits in enumerate(prediction):\n",
    "    # Record the class index with maximum probability.\n",
    "    class_index = tf.argmax(logits).numpy()\n",
    "    # Calculate the class softmax probability.\n",
    "    probability = tf.nn.softmax(logits)[class_index] * 10\n",
    "    # Display the index, class name, and predicted class probability\n",
    "    name = class_name[class_index]\n",
    "    print (\"index {}, class: {}, probability {:2.2%}\".format(index,name,probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below denotes the model predict results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|![RESULTS](/images/predict.png)|\n",
    "|:---|\n"
    "||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
